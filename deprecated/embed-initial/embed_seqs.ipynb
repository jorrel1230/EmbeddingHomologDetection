{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed Sequences\n",
    "\n",
    "The primary purpose of this notebook is to take the SCOP CSV of sequences and embed them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import faiss\n",
    "import esm\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Mapping: Contains unique integer index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>fa</th>\n",
       "      <th>sf</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Q03131</td>\n",
       "      <td>4000119</td>\n",
       "      <td>3000038</td>\n",
       "      <td>MSGPRSRTTSRRTPVRIGAVVVASSTSELLDGLAAVADGRPHASVV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>P09147</td>\n",
       "      <td>4000088</td>\n",
       "      <td>3000038</td>\n",
       "      <td>MRVLVTGGSGYIGSHTCVQLLQNGHDVIILDNLCNSKRSVLPVIER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>P61889</td>\n",
       "      <td>4000045</td>\n",
       "      <td>3000039</td>\n",
       "      <td>MKVAVLGAAGGIGQALALLLKTQLPSGSELSLYDIAPVTPGVAVDL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>P00334</td>\n",
       "      <td>4000029</td>\n",
       "      <td>3000038</td>\n",
       "      <td>MSFTLTNKNVIFVAGLGGIGLDTSKELLKRDLKNLVILDRIENPAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>O33830</td>\n",
       "      <td>4000089</td>\n",
       "      <td>3000039</td>\n",
       "      <td>MPSVKIGIIGAGSAVFSLRLVSDLCKTPGLSGSTVTLMDIDEERLD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35972</th>\n",
       "      <td>35972</td>\n",
       "      <td>P20585</td>\n",
       "      <td>4004015</td>\n",
       "      <td>3000587</td>\n",
       "      <td>MSRRKPASGGLAASSSAPARQAVLSRFFQSTGSLKSTSSSTGAADQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35973</th>\n",
       "      <td>35973</td>\n",
       "      <td>P20585</td>\n",
       "      <td>4004015</td>\n",
       "      <td>3002020</td>\n",
       "      <td>MSRRKPASGGLAASSSAPARQAVLSRFFQSTGSLKSTSSSTGAADQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35974</th>\n",
       "      <td>35974</td>\n",
       "      <td>P52701</td>\n",
       "      <td>4004015</td>\n",
       "      <td>3001688</td>\n",
       "      <td>MSRQSTLYSFFPKSPALSDANKASARASREGGRAAAAPGASPSPGG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35975</th>\n",
       "      <td>35975</td>\n",
       "      <td>P52701</td>\n",
       "      <td>4004015</td>\n",
       "      <td>3000587</td>\n",
       "      <td>MSRQSTLYSFFPKSPALSDANKASARASREGGRAAAAPGASPSPGG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35976</th>\n",
       "      <td>35976</td>\n",
       "      <td>P52701</td>\n",
       "      <td>4004015</td>\n",
       "      <td>3002020</td>\n",
       "      <td>MSRQSTLYSFFPKSPALSDANKASARASREGGRAAAAPGASPSPGG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35977 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index     uid       fa       sf  \\\n",
       "0          0  Q03131  4000119  3000038   \n",
       "1          1  P09147  4000088  3000038   \n",
       "2          2  P61889  4000045  3000039   \n",
       "3          3  P00334  4000029  3000038   \n",
       "4          4  O33830  4000089  3000039   \n",
       "...      ...     ...      ...      ...   \n",
       "35972  35972  P20585  4004015  3000587   \n",
       "35973  35973  P20585  4004015  3002020   \n",
       "35974  35974  P52701  4004015  3001688   \n",
       "35975  35975  P52701  4004015  3000587   \n",
       "35976  35976  P52701  4004015  3002020   \n",
       "\n",
       "                                                     seq  \n",
       "0      MSGPRSRTTSRRTPVRIGAVVVASSTSELLDGLAAVADGRPHASVV...  \n",
       "1      MRVLVTGGSGYIGSHTCVQLLQNGHDVIILDNLCNSKRSVLPVIER...  \n",
       "2      MKVAVLGAAGGIGQALALLLKTQLPSGSELSLYDIAPVTPGVAVDL...  \n",
       "3      MSFTLTNKNVIFVAGLGGIGLDTSKELLKRDLKNLVILDRIENPAA...  \n",
       "4      MPSVKIGIIGAGSAVFSLRLVSDLCKTPGLSGSTVTLMDIDEERLD...  \n",
       "...                                                  ...  \n",
       "35972  MSRRKPASGGLAASSSAPARQAVLSRFFQSTGSLKSTSSSTGAADQ...  \n",
       "35973  MSRRKPASGGLAASSSAPARQAVLSRFFQSTGSLKSTSSSTGAADQ...  \n",
       "35974  MSRQSTLYSFFPKSPALSDANKASARASREGGRAAAAPGASPSPGG...  \n",
       "35975  MSRQSTLYSFFPKSPALSDANKASARASREGGRAAAAPGASPSPGG...  \n",
       "35976  MSRQSTLYSFFPKSPALSDANKASARASREGGRAAAAPGASPSPGG...  \n",
       "\n",
       "[35977 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scop_csv_path = '/scratch/gpfs/jr8867/datasets/scop/scop_data.csv'\n",
    "scop_df = pd.read_csv(scop_csv_path)\n",
    "scop_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the  Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA available:\", cuda_available)\n",
    "device = torch.device(\"cuda\" if cuda_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0-32): 33 x TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.to(device)\n",
    "model.eval() # Ensures that the model is in evaluation mode, not training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear CUDA cache to free up memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated?\n",
    "def embed_sequence(sequence):\n",
    "    \"\"\"\n",
    "    Convert a protein sequence to embedding using ESM-2 with mean pooling.\n",
    "    \n",
    "    Args:\n",
    "    - sequence (str): A protein sequence as string.\n",
    "\n",
    "    Returns:\n",
    "    - embedding (np.array): Array of shape (D,) where D is embedding size.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize sequence\n",
    "    data = [(str(0), sequence)]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "    # Move to GPU if available\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "    # Forward pass to get embeddings\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "\n",
    "    # Extract embedding (mean pooling excluding padding)\n",
    "    token_embeddings = results[\"representations\"][33]  # Extract last hidden layer (layer 33 for this model)\n",
    "    \n",
    "    # Apply mean pooling\n",
    "    valid_tokens = batch_tokens[0] != alphabet.padding_idx  # Mask out padding tokens\n",
    "    seq_embedding = token_embeddings[0, valid_tokens].mean(dim=0).cpu().numpy()\n",
    "\n",
    "    return seq_embedding\n",
    "\n",
    "def embed_sequence_batch(sequences, batch_size=8):\n",
    "    \"\"\"\n",
    "    Convert a batch of protein sequences to embeddings using ESM-2 with mean pooling.\n",
    "    \n",
    "    Args:\n",
    "    - sequences (list): List of protein sequences as strings.\n",
    "    - batch_size (int): Number of sequences to process at once.\n",
    "\n",
    "    Returns:\n",
    "    - embeddings (np.array): Array of shape (N, D) where N is number of sequences, D is embedding size.\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "\n",
    "    sub_batch_size = 1\n",
    "    \n",
    "    # Process in sub-batches to avoid memory issues\n",
    "    for i in range(0, len(sequences), sub_batch_size):\n",
    "        sub_batch_seqs = sequences[i:i+sub_batch_size]\n",
    "        sub_batch_data = [(str(j), seq) for j, seq in enumerate(sub_batch_seqs)]\n",
    "        \n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter(sub_batch_data)\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        \n",
    "        # Clear CUDA cache to free up memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "        \n",
    "        token_embeddings = results[\"representations\"][33]\n",
    "        \n",
    "        # Apply mean pooling for each sequence in the sub-batch\n",
    "        for j in range(len(sub_batch_seqs)):\n",
    "            valid_tokens = batch_tokens[j] != alphabet.padding_idx\n",
    "            seq_embedding = token_embeddings[j, valid_tokens].mean(dim=0).cpu().numpy()\n",
    "            all_embeddings.append(seq_embedding)\n",
    "        \n",
    "        # Clear CUDA cache again\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return np.array(all_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_and_save_embeddings(df, output_dir, batch_size=16, save_every=100):\n",
    "    \"\"\"\n",
    "    Process sequences in batches, update FAISS index, and save periodically.\n",
    "    Uses the existing 'index' column from the DataFrame as FAISS indices.\n",
    "    \n",
    "    Args:\n",
    "    - df (DataFrame): DataFrame containing sequences, metadata, and an 'index' column\n",
    "    - output_dir (str): Directory to save FAISS index and metadata\n",
    "    - batch_size (int): Number of sequences to process in each batch for FAISS updates\n",
    "    - save_every (int): Save after processing this many sequences\n",
    "    \n",
    "    Returns:\n",
    "    - index: Final FAISS index\n",
    "    - metadata: DataFrame with metadata\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Setup FAISS index with explicit IDs\n",
    "    dimension = 1280  # ESM-2 embedding dimension\n",
    "    index = faiss.IndexIDMap(faiss.IndexFlatL2(dimension))\n",
    "    \n",
    "    # Process in batches\n",
    "    total_processed = 0\n",
    "    \n",
    "    for i in tqdm(range(0, len(df), batch_size), ncols=100):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Get embeddings for this batch\n",
    "        batch_embeddings = embed_sequence_batch(batch_df['seq'].tolist())\n",
    "        \n",
    "        # Get the original indices from the DataFrame\n",
    "        batch_indices = batch_df['index'].values\n",
    "        \n",
    "        # Add to FAISS index with explicit IDs\n",
    "        index.add_with_ids(\n",
    "            batch_embeddings.astype('float32'),\n",
    "            np.array(batch_indices, dtype=np.int64)\n",
    "        )\n",
    "        \n",
    "        total_processed += len(batch_embeddings)\n",
    "        \n",
    "        # Save periodically\n",
    "        if total_processed % save_every == 0 or i + batch_size >= len(df):\n",
    "            faiss_path = os.path.join(output_dir, 'protein_embeddings.index')\n",
    "            \n",
    "            # Save FAISS index\n",
    "            faiss.write_index(index, faiss_path)\n",
    "            \n",
    "            # No need to save metadata separately since we're using the original indices\n",
    "            print(f\"Saved {total_processed} embeddings to {faiss_path}\")\n",
    "            print(f\"Using original DataFrame indices as FAISS IDs\")\n",
    "    \n",
    "    return index, df  # Return the original DataFrame as metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "output_dir = '/scratch/gpfs/jr8867/embeddings/scop'\n",
    "batch_size = 16  # Batch size for FAISS updates\n",
    "save_every = 100  # Save after processing this many sequences\n",
    "\n",
    "# Process all sequences\n",
    "index, metadata = process_and_save_embeddings(scop_df, output_dir, batch_size, save_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search for similar proteins\n",
    "def search_similar_proteins(query_seq, index, metadata, k=5):\n",
    "    \"\"\"\n",
    "    Search for similar proteins using a query sequence.\n",
    "    \n",
    "    Args:\n",
    "    - query_seq (str): Query protein sequence\n",
    "    - index: FAISS index\n",
    "    - metadata (DataFrame): Metadata DataFrame\n",
    "    - k (int): Number of nearest neighbors to return\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Metadata of similar proteins\n",
    "    \"\"\"\n",
    "    # Embed query sequence\n",
    "    query_embedding = embed_sequence(query_seq)\n",
    "    query_embedding = np.array([query_embedding]).astype('float32')\n",
    "    \n",
    "    # Search in FAISS index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    \n",
    "    # Get metadata for results\n",
    "    result_indices = I[0]\n",
    "    similar_proteins = metadata[metadata['index'].isin(result_indices)]\n",
    "    \n",
    "    # Add distance information\n",
    "    distances = {idx: dist for idx, dist in zip(result_indices, D[0])}\n",
    "    similar_proteins['distance'] = similar_proteins['index'].map(distances)\n",
    "    \n",
    "    # Sort by distance\n",
    "    similar_proteins = similar_proteins.sort_values('distance')\n",
    "    \n",
    "    return similar_proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar proteins:\n",
      "    index     uid       fa       sf  \\\n",
      "0       0  Q03131  4000119  3000038   \n",
      "32     32  Q59771  4000004  3000046   \n",
      "23     23  P9WNX3  4000051  3000006   \n",
      "22     22  P9WNX3  4000099  3000044   \n",
      "24     24  P9WNX3  4000037  3000019   \n",
      "\n",
      "                                                  seq  distance  \n",
      "0   MSGPRSRTTSRRTPVRIGAVVVASSTSELLDGLAAVADGRPHASVV...  0.000000  \n",
      "32  MSIDSALNWDGEMTVTRFDRETGAHFVIRLDSTQLGPAAGGTRAAQ...  3.769551  \n",
      "23  MSLPVVLIADKLAPSTVAALGDQVEVRWVDGPDRDKLLAAVPEADA...  4.216374  \n",
      "22  MSLPVVLIADKLAPSTVAALGDQVEVRWVDGPDRDKLLAAVPEADA...  4.216374  \n",
      "24  MSLPVVLIADKLAPSTVAALGDQVEVRWVDGPDRDKLLAAVPEADA...  4.216374  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1930496/3445028982.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_proteins['distance'] = similar_proteins['index'].map(distances)\n"
     ]
    }
   ],
   "source": [
    "example_seq = scop_df.iloc[0]['seq']\n",
    "index = faiss.read_index('/scratch/gpfs/jr8867/embeddings/scop/protein_embeddings.index')\n",
    "similar = search_similar_proteins(example_seq, index, scop_df)\n",
    "print(\"Similar proteins:\")\n",
    "print(similar)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
